
新论文✅ 
6月我们发布了「大锁定假说」。人机交互会使得人类整体陷入“地心说”式的错误信念而难以逃离吗？
我们发现持续的人机交互使得人类整体会固化当前世界的信念(beliefs) 和价值观。这是由于人类向大语言模型学习，而大语言模型的数据又来自人类，这样的反馈闭环会让个体无意识间重复暴露于已有信念，但却强化了信心。
我们称之为“大锁定假说”。
文章中了ICML，我们用形式化方法、多智能体模拟、WildChat数据分析三种方式研究了大锁定假说：
- 通过formal methods我们发现，当反馈闭环存在时，只要人机之间有"moderate mutual trust"，锁定几乎必然发生（如图二c所示，多轮交互后，集体信念永久性地和正确答案偏离）；
- 通过simulations我们验证了这个直觉：即长期的人机交互使得集体信念的多样化逐渐丧失，锁定开始发生，即集体信念变得单一（如图三所示）;
- 通过wildchat data analysis （即GPT的镜像服务），每一次新版本发布时，都用统计显著的人机交互内容多样性丧失。即用LLM实时交互数据再训练模型后，集体信念开始丧失多样性（如图4所示）。
文章呈现了早期证据支持大锁定假说。我们希望持续的研究能避免长期使用大语言模型使得人类走入类似“地心说”式的困境。
https://arxiv.org/pdf/2506.06166
[以上内容来源于Zhonghao He的朋友圈]
这是剑桥大学何忠豪他们团队最新的工作。“大锁定假说”很有意思，我个人感觉可以作为后续当电力系统大量使用LLM后，如何更好地给出正确信息或者答案的问题背景，希望对老师你们的工作也有启发。
